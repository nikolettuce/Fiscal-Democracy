{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Parsing (Summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard imports, add as needed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets file directory list, removes all duplicates in directory\n",
    "filedir = os.listdir('Data')\n",
    "filedir = [file for file in filedir if (\n",
    "    ('(1)' and 'copy' not in file) and (file[-3:] == 'txt'))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pattern matching all the for arguments\n",
    "pattern_sum = r'(?i)(?=\\b(Measure|Impartial|Analysis|Counsel|Auditor)\\b).*?(?=\\b(Counsel|Analysis)\\b)\\w*'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make first pass through the data, and extract the summaries\n",
    "\n",
    "Process:\n",
    "- For each file in filedir, match the line after the Impartial Analysis heading\n",
    "- Then take the next 500 \"words\" and then end the string\n",
    "- Store for analysis, and continue iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for re-usability in next portions of project\n",
    "def match_maker(fdir, pattern):\n",
    "    match = []\n",
    "\n",
    "    for f in fdir:\n",
    "        # read file\n",
    "        fp = os.path.join('Data', f)\n",
    "        file = open(fp, 'r', encoding = \"utf8\")\n",
    "        f_text = file.read()\n",
    "\n",
    "        # use regex to match the first instance of regex, then read the next 500 words.\n",
    "        regex_match = re.search(pattern, f_text)\n",
    "\n",
    "        # if regex matched\n",
    "        if regex_match:\n",
    "            f_text = f_text[regex_match.end():]\n",
    "            # cleaning text\n",
    "            for char in ['-','\\n']:\n",
    "                f_text = f_text.replace(char,' ')\n",
    "\n",
    "            f_split = f_text.split()\n",
    "            # debugger, ignore : print(f + \" is this long: \" + str(len(f_split)))\n",
    "            # Some matches won't have 500 characters following\n",
    "            if len(f_split) <= 500:\n",
    "                f_processed = \" \".join(f_split[:(len(f_split) - 1)])\n",
    "            else:\n",
    "                f_processed = \" \".join(f_split[:500])\n",
    "\n",
    "            # add to list\n",
    "            match.append((f, f_processed))\n",
    "        else:\n",
    "            match.append((f, 'NO MATCH'))\n",
    "        file.close()\n",
    "        \n",
    "    return match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use this function to match all of the Impartial Analyses!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pattern for summary defined previously\n",
    "match_sum = match_maker(filedir, pattern_sum)\n",
    "\n",
    "pct_matched = len([i for i in match_sum if i[1] != 'NO MATCH'])/len(match_sum)\n",
    "print('We matched ' + str(pct_matched*100) + \"% of the Impartial Analyses!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But the Arguments are a bit harder....\n",
    "\n",
    "Plan of Action:\n",
    "- Let's filter out the file types that are a majority argumentless\n",
    "- Design a pattern to match most of the arguments for\n",
    "- Match arguments for\n",
    "- Compare to master list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter our sample ballot files\n",
    "filter_list = [\"SampleBallot\", \"Res.\", \"BQ\", 'bq', \"Bq\", \"Ord\", \"BallotQ\"]\n",
    "\n",
    "# define to use as a filter for other parts of project\n",
    "def list_filter(fdir, fil):\n",
    "    return [f for f in fdir if any(l for l in fil if l in f) == False]        \n",
    "        \n",
    "filedir_args = list_filter(filedir, filter_list)\n",
    "\n",
    "len(filedir), len(filedir_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like it worked! Now lets match them :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_for = r'(?i)((?!.*?(REBUTTAL).*?)(?=.*\\b(ARGUMENT))(?=.*(FAVOR|FOR)).*)'\n",
    "match_for = match_maker(filedir_args, pattern_for)\n",
    "\n",
    "total_for = len([i for i in match_for if i[1] != 'NO MATCH'])\n",
    "pct_matched_for = total_for/len(match_for)\n",
    "print('We matched ' + str(pct_matched_for * 100) + \"% of the docs!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compare our results with the master list\n",
    "master_df = pd.read_csv('MasterList2019Nov14.csv')\n",
    "# Fill all NaNs with 0, and replace 5's (Never Existed) with 0's.\n",
    "df_for = master_df.For_Coll.fillna(0).replace(5, 0)\n",
    "df_for.value_counts().loc[0], total_for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we matched about 50% of the For arguments according to the master list, but about 88% of the For arguments from\n",
    "our filtered list. \n",
    "\n",
    "When we use the entire file directory, we get about 75% matches on that. It is still uncertain whether one with a higher likelihood\n",
    "to have false positives is better than a smaller, higher accuracy and higher precision data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helpme= match_maker(filedir, pattern_for)\n",
    "#helpme_for = len([i for i in helpme if i[1] != 'NO MATCH'])\n",
    "#helpme_matched_for = helpme_for/len(helpme)\n",
    "#print('We matched ' + str(helpme_matched_for * 100) + \"% of the docs!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#pattern_against = r'(?i)((?!.*?(REBUTTAL).*?)(?=.*\\b(ARGUMENT))(?=.*(AGAINST|OPPOSING)).*)'\n",
    "\n",
    "#match_against = match_maker(filedir_args, pattern_against)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pct_matched = len([i for i in match_against if i[1] != 'NO MATCH'])/len(match_against)\n",
    "#print('We matched ' + str(pct_matched*100) + \"% of the docs!\")\n",
    "#len([i for i in match_against if i[1] != 'NO MATCH'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Monica's attempts starts from here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_for = []\n",
    "for index in range(len(match_for)):\n",
    "    text_for = match_for[index][1]\n",
    "    list_for.append(text_for)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(list_for) == 1218\n",
    "assert type(text_for) == str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str.maketrans?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stripping punctuation not working\n",
    "#for_text = [re.sub(r'[^A-Za-z0-9]','', text_for) for text_for in list_for]\n",
    "#for_text = [re.sub(r'[^\\w\\s]','',text_for) for text_for in list_for]\n",
    "#for_text = [re.sub('\\W+','',text_for) for text_for in list_for]\n",
    "\n",
    "intab = \"'■*...■■r■■£►♦❖'!`?,;:._'#^<>\"\n",
    "#print(len(intab))\n",
    "outtab = \"                             \"\n",
    "for_text = [x.translate(str.maketrans(intab, outtab, string.punctuation)) for x in list_for]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying to strip text of punctuation and lower all words\n",
    "\n",
    "for_text = list(map(str.lower, list_for))\n",
    "\n",
    "for_text[199]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(sublinear_tf=True,\n",
    "                        analyzer='word',\n",
    "                        max_features=10000,\n",
    "                        tokenizer=word_tokenize,\n",
    "                        stop_words=stopwords.words(\"english\"),\n",
    "                        max_df = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TfidfVectorizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_tfidf = pd.DataFrame(tfidf.fit_transform(list_for).toarray())\n",
    "for_tfidf.columns = tfidf.get_feature_names()\n",
    "for_tfidf.index = filedir_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_unique = for_tfidf.idxmax(axis=1)\n",
    "most_unique[511]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_tfidf.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_sum = []\n",
    "for index in range(len(match_sum)):\n",
    "    text_sum = match_sum[index][1]\n",
    "    list_sum.append(text_sum)\n",
    "    \n",
    "intab = \"'■*...■■r■■£►♦❖'!`?,;:._'#^<>\"\n",
    "outtab = \"                             \"\n",
    "sum_text = [x.translate(str.maketrans(intab, outtab, string.punctuation)) for x in list_sum]\n",
    "\n",
    "sum_text = list(map(str.lower, list_sum))\n",
    "sum_text[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_tfidf = pd.DataFrame(tfidf.fit_transform(list_sum).toarray())\n",
    "sum_tfidf.columns = tfidf.get_feature_names()\n",
    "sum_tfidf.index = filedir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_most_unique = sum_tfidf.idxmax(axis=1)\n",
    "sum_most_unique[899]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pivot to readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from readability import Readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dale Chall Readabiity for Arguments for\n",
    "for index in range(len(list_for)):\n",
    "    try:\n",
    "        text = list_for[index]\n",
    "        title = filedir_args[index]\n",
    "        \n",
    "        r = Readability(text)\n",
    "        dc_score = r.dale_chall()\n",
    "        \n",
    "        print(title, dc_score)\n",
    "        \n",
    "        \n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gunning Fog Readability for Arguments for\n",
    "for index in range(len(list_for)):\n",
    "    try:\n",
    "        text = list_for[index]\n",
    "        title = filedir_args[index]\n",
    "        \n",
    "        r = Readability(text)\n",
    "        gf_score = r.gunning_fog()\n",
    "\n",
    "        print(title, gf_score)\n",
    "        \n",
    "        \n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dale Chall Readability for Summaries\n",
    "for index in range(len(list_sum)):\n",
    "    try:\n",
    "        text = list_sum[index]\n",
    "        title = filedir[index]\n",
    "        \n",
    "        r = Readability(text)\n",
    "        dc_score = r.dale_chall()\n",
    "        \n",
    "        print(title, dc_score)\n",
    "        \n",
    "        \n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gunning Fog Readability for Summaries\n",
    "for index in range(len(list_sum)):\n",
    "    try:\n",
    "        text = list_sum[index]\n",
    "        title = filedir[index]\n",
    "        \n",
    "        r = Readability(text)\n",
    "        gf_score = r.gunning_fog()\n",
    "        \n",
    "        print(title, gf_score)\n",
    "        \n",
    "        \n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
